{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6vZfOHWoYPj"
      },
      "source": [
        "# **Lab Sheet: Text Mining Preprocessing with Python**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNCf5ILGod9J"
      },
      "source": [
        "**Objectives**\n",
        "\n",
        "1.   Implement basic text preprocessing techniques.\n",
        "2.Clean and prepare text data for further analysis.\n",
        "2.   Use Bag of Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF) models to vectorize text data.\n",
        "3.   Understand and apply common text mining techniques.\n",
        "5.Use libraries such as nltk and re to perform common preprocessing tasks.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlsL5fRqpJtn"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1mtG5nSpfpy"
      },
      "source": [
        "**Prerequisites**\n",
        "\n",
        "*   Basic knowledge of Python programming.\n",
        "*   Understanding of text processing concepts.\n",
        "* Python installed on your machine.\n",
        "* Internet connection to download necessary libraries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6von8yGwpyOu"
      },
      "source": [
        "**Instructions**\n",
        "\n",
        "**1. Set Up Your Environment**\n",
        "1. Install Python: Make sure you have Python 3.x installed. You can download it from python.org.\n",
        "\n",
        "2. Create and Activate a Virtual Environment (optional but recommended):\n",
        "\n",
        "* Create a virtual environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZImJVyuPp_An"
      },
      "outputs": [],
      "source": [
        "python -m venv myenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3sc6wXOqCNk"
      },
      "source": [
        "* Activate the virtual environment\n",
        "  * On Windows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZodQjtpLqMdo"
      },
      "outputs": [],
      "source": [
        "myenv\\Scripts\\activate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4zs7_FIqPL8"
      },
      "source": [
        "   * On macOS/Linux"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgroKgZfoR_O"
      },
      "outputs": [],
      "source": [
        "source myenv/bin/activate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6pyMCvsqbWs"
      },
      "source": [
        "3. Install Required Libraries:\n",
        "* Install 'nltk'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZEqwH-xqiAk",
        "outputId": "e8e877fd-c7a2-4352-c68f-dc25ddb2e2e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqU2lIubql-w"
      },
      "source": [
        "**2. Download NLTK Resources**\n",
        "\n",
        "You need to download specific resources from 'nltk' to use stopwords and tokenizers. Run the following code to download these resources:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9REAMWKqrNo",
        "outputId": "9921c7a2-d350-4090-cb9f-527a52ce6408"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\jasle\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\jasle\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGblDLqBqthU"
      },
      "source": [
        "**3. Implement the Preprocessing Code**\n",
        "\n",
        "Create a Python script or Jupyter Notebook with the following code to perform text preprocessing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94-YPJfyqzxQ",
        "outputId": "c8c258ef-db7b-4a98-8d3e-0d3d844105d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['hello', 'world', 'exampl', 'text', 'preprocess', 'python', 'let', 'clean', 'text']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Initialize the stemmer and stopwords\n",
        "stemmer = PorterStemmer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and perform stemming\n",
        "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Sample text\n",
        "text = \"Hello world! This is an example of text preprocessing in Python. Let's clean this text.\"\n",
        "\n",
        "# Preprocess the sample text\n",
        "preprocessed_tokens = preprocess_text(text)\n",
        "print(preprocessed_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ENrOledq1j8"
      },
      "source": [
        "4. **Run the Code**\n",
        "\n",
        "Execute the script or notebook to see the output of the 'preprocessing steps'. The preprocessed_tokens variable should contain the cleaned and processed tokens from the sample text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUpab2Hrq94G"
      },
      "source": [
        "5. **Implement Bag of Words (BoW)**\n",
        "\n",
        "Add the following code to convert the preprocessed text into a Bag of Words model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwV7ch_5rGD8",
        "outputId": "ec9a92b5-7bb7-4d79-ecde-35421eb2c90d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature Names: ['analysis' 'aspect' 'clean' 'data' 'example' 'hello' 'important' 'let'\n",
            " 'mining' 'prepare' 'preprocessing' 'text' 'world']\n",
            "Document Matrix (BoW):\n",
            "[[0 0 0 0 1 1 0 0 0 0 1 1 1]\n",
            " [1 1 0 1 0 0 1 0 1 0 0 1 0]\n",
            " [1 0 1 1 0 0 0 1 0 1 0 1 0]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"Hello world! This is an example of text preprocessing.\",\n",
        "    \"Text mining is an important aspect of data analysis.\",\n",
        "    \"Let's clean and prepare text data for analysis.\"\n",
        "]\n",
        "\n",
        "# Initialize CountVectorizer\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "\n",
        "# Fit and transform the documents\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Get feature names and converted document matrix\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "document_matrix = X.toarray()\n",
        "\n",
        "print(\"Feature Names:\", feature_names)\n",
        "print(\"Document Matrix (BoW):\")\n",
        "print(document_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rI7rU_V0rK8g"
      },
      "source": [
        "**6. Implement TF-IDF**\n",
        "\n",
        "Add the following code to convert the text into a TF-IDF representation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kR0SfdlrI5V",
        "outputId": "1d2f825a-da7e-4bc9-da01-864cddc2007c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature Names (TF-IDF): ['analysis' 'aspect' 'clean' 'data' 'example' 'hello' 'important' 'let'\n",
            " 'mining' 'prepare' 'preprocessing' 'text' 'world']\n",
            "Document Matrix (TF-IDF):\n",
            "[[0.         0.         0.         0.         0.47952794 0.47952794\n",
            "  0.         0.         0.         0.         0.47952794 0.28321692\n",
            "  0.47952794]\n",
            " [0.35829137 0.4711101  0.         0.35829137 0.         0.\n",
            "  0.4711101  0.         0.4711101  0.         0.         0.27824521\n",
            "  0.        ]\n",
            " [0.35829137 0.         0.4711101  0.35829137 0.         0.\n",
            "  0.         0.4711101  0.         0.4711101  0.         0.27824521\n",
            "  0.        ]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "# Fit and transform the documents\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "# Get feature names and converted document matrix\n",
        "feature_names_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
        "document_matrix_tfidf = X_tfidf.toarray()\n",
        "\n",
        "print(\"Feature Names (TF-IDF):\", feature_names_tfidf)\n",
        "print(\"Document Matrix (TF-IDF):\")\n",
        "print(document_matrix_tfidf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWQ96dS8rUeI"
      },
      "source": [
        "***Exercises***\n",
        "\n",
        "* Implement Lemmatization: Replace the stemming process with lemmatization using WordNetLemmatizer.\n",
        "* Handle Different Languages: Modify the code to preprocess text in a different language using the appropriate stopwords and tokenizers.\n",
        "\n",
        "* Explore Other Vectorizers: Try using HashingVectorizer and compare it with CountVectorizer and TfidfVectorizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\jasle\\AppData\\Roaming\\nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', 'world', '!', 'This', 'is', 'an', 'example', 'of', 'text', 'lemmatization', 'in', 'Python', '.', 'Let', \"'s\", 'clean', 'this', 'text', '.']\n"
          ]
        }
      ],
      "source": [
        "# Implementing Lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')  \n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_text(text):\n",
        "  # Tokenize the text\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  # Lemmatize the tokens\n",
        "  lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "  return lemmatized_tokens\n",
        "\n",
        "# Sample text\n",
        "text = \"Hello world! This is an example of text lemmatization in Python. Let's clean this text.\"\n",
        "\n",
        "# Lemmatize the sample text\n",
        "lemmatized_tokens = lemmatize_text(text)\n",
        "\n",
        "print(lemmatized_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['bonjour', 'mond', 'cec', 'exempl', 'prétrait', 'text', 'python', 'nettoyon', 'text']\n"
          ]
        }
      ],
      "source": [
        "# Handle different languages\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Initialize the stemmer and stopwords for French\n",
        "stemmer_fr = SnowballStemmer('french')\n",
        "stop_words_fr = set(stopwords.words('french'))\n",
        "\n",
        "def preprocess_text_fr(text):\n",
        "  # Convert text to lowercase\n",
        "  text = text.lower()\n",
        "\n",
        "  # Remove punctuation\n",
        "  text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "  # Tokenize the text\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  # Remove stopwords and perform stemming\n",
        "  tokens = [stemmer_fr.stem(word) for word in tokens if word not in stop_words_fr]\n",
        "\n",
        "  return tokens\n",
        "\n",
        "# Sample French text\n",
        "text_fr = \"Bonjour le monde! Ceci est un exemple de prétraitement de texte en Python. Nettoyons ce texte.\"\n",
        "\n",
        "# Preprocess the sample French text\n",
        "preprocessed_tokens_fr = preprocess_text_fr(text_fr)\n",
        "\n",
        "print(preprocessed_tokens_fr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document Matrix (Hashing):\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "Feature Names (TF-IDF with n-grams): ['analysis' 'aspect' 'aspect data' 'clean' 'clean prepare' 'data'\n",
            " 'data analysis' 'example' 'example text' 'hello' 'hello world'\n",
            " 'important' 'important aspect' 'let' 'let clean' 'mining'\n",
            " 'mining important' 'prepare' 'prepare text' 'preprocessing' 'text'\n",
            " 'text data' 'text mining' 'text preprocessing' 'world' 'world example']\n",
            "Document Matrix (TF-IDF with n-grams):\n",
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.34608857 0.34608857 0.34608857 0.34608857 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.34608857 0.20440549 0.         0.         0.34608857\n",
            "  0.34608857 0.34608857]\n",
            " [0.25233341 0.33178811 0.33178811 0.         0.         0.25233341\n",
            "  0.25233341 0.         0.         0.         0.         0.33178811\n",
            "  0.33178811 0.         0.         0.33178811 0.33178811 0.\n",
            "  0.         0.         0.1959594  0.         0.33178811 0.\n",
            "  0.         0.        ]\n",
            " [0.25233341 0.         0.         0.33178811 0.33178811 0.25233341\n",
            "  0.25233341 0.         0.         0.         0.         0.\n",
            "  0.         0.33178811 0.33178811 0.         0.         0.33178811\n",
            "  0.33178811 0.         0.1959594  0.33178811 0.         0.\n",
            "  0.         0.        ]]\n"
          ]
        }
      ],
      "source": [
        "# Other vectorizers - HashingVectorizer and TfidfVectorizer\n",
        "\n",
        "from sklearn.feature_extraction.text import HashingVectorizer, TfidfVectorizer\n",
        "\n",
        "# Initialize HashingVectorizer\n",
        "hashing_vectorizer = HashingVectorizer(stop_words='english')\n",
        "\n",
        "# Fit and transform the documents\n",
        "X_hashing = hashing_vectorizer.fit_transform(documents)\n",
        "\n",
        "# Get converted document matrix\n",
        "document_matrix_hashing = X_hashing.toarray()\n",
        "\n",
        "print(\"Document Matrix (Hashing):\")\n",
        "print(document_matrix_hashing)\n",
        "\n",
        "# Initialize TfidfVectorizer with n-grams\n",
        "tfidf_vectorizer_ngram = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))\n",
        "\n",
        "# Fit and transform the documents\n",
        "X_tfidf_ngram = tfidf_vectorizer_ngram.fit_transform(documents)\n",
        "\n",
        "# Get feature names and converted document matrix\n",
        "feature_names_tfidf_ngram = tfidf_vectorizer_ngram.get_feature_names_out()\n",
        "document_matrix_tfidf_ngram = X_tfidf_ngram.toarray()\n",
        "\n",
        "print(\"Feature Names (TF-IDF with n-grams):\", feature_names_tfidf_ngram)\n",
        "print(\"Document Matrix (TF-IDF with n-grams):\")\n",
        "print(document_matrix_tfidf_ngram)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
